---
title: "IMDB-RNN"
author: "Saleh Alshehri"
date: "3/3/2020"
output: html_document
---
```{r}
library(keras)

# Processing the labels of the raw IMDB data
imdb_dir <- "C:/Users/Saleh/Downloads/aclImdb"
train_dir <- file.path(imdb_dir, "train")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}

# cut-off reviews after 150 words
maxlen <- 150
# restricting the training_data to first 100 samples
training_samples <- 100
# validates on 10000 samples
validation_samples <- 10000
# considering only top 10,000 words in the dataset
max_words <- 10000

# tokenizing the words
tokenizer <- text_tokenizer(num_words = max_words) %>%
  fit_text_tokenizer(texts)
sequences <- texts_to_sequences(tokenizer, texts)
word_index = tokenizer$word_index


# Turns the list of integers into a 2D integer tensor shape (samples,maxlen)
data <- pad_sequences(sequences, maxlen = maxlen)
labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat('Shape of label tensor:', dim(labels), "\n")
set.seed(123)
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1):
                                (training_samples + validation_samples)]

train_data <- data[training_indices,]
train_label <- labels[training_indices]
valid_data <- data[validation_indices,]
valid_label<- labels[validation_indices]

test_dir <- file.path(imdb_dir, "test")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(test_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
sequences <- texts_to_sequences(tokenizer, texts)
x_test <- pad_sequences(sequences, maxlen = maxlen)
y_test <- as.array(labels)

# Using an embedding layer and classifier on the IMDB data
model <- keras_model_sequential() %>% layer_embedding(input_dim = 10000,output_dim = 8,input_length = maxlen) %>% 
  layer_flatten() %>% layer_dense(units=1,activation = "sigmoid")
model %>% compile(optimizer = "rmsprop",loss = "binary_crossentropy",metrics=c("acc"))

history <- model %>% fit(train_data,train_label,epochs=10,batch_size=32,validation_data = list(valid_data,valid_label))
# Plot of Accuracy and Loss function of the model
plot(history)
# By observing the plot, the validation accuracy of the model is ~50% considering the first 150 words in every review with 100 samples.

# Evaluating the test dataset 
model %>% fit(
  train_data,
  train_label,
  epochs = 2,
  batch_size = 32)
result <- model %>%  evaluate(x_test,y_test)
result
cat("The Test accuracy of the model is ",result$acc)

# Parsing the GloVe word-embeddings file
glove_dir = 'C:/Users/Vijay/Downloads/glove.6B'
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}
cat("Found", length(embeddings_index), "word vectors.\n")


# Preparing the GloVe word-embeddings matrix
embedding_dim <- 100
embedding_matrix <- array(0, c(max_words, embedding_dim))
for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      embedding_matrix[index+1,] <- embedding_vector
  }
}

# Model construction
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words, output_dim = embedding_dim,input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

# Loading pretrained word embeddings into the embedding layer
get_layer(model, index = 1) %>%
  set_weights(list(embedding_matrix)) %>%
  freeze_weights()

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history1 <- model %>% fit(
  train_data, train_label,
  epochs = 20,
  batch_size = 32,
  validation_data = list(valid_data , valid_label)
)
plot(history1)
# By observing the above plot, the validaition accuracy of the model is ~50% with 100 samples in the training dataset.THe model quickly starts overfitting with small number of traning samples. Hence with having few traning samples, performance is highly dependent on exactly which 100 samples are choosen and choosing at random. 


model %>% fit(
  train_data, train_label,
  epochs = 2,
  batch_size = 32)
result1 <- model %>% evaluate(x_test,y_test)
result1
cat("The Test accuracy of the model is ",result1$acc)





```

